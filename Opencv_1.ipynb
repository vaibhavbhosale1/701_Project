{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eec8d37-8c2b-4321-886a-51a932a62136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa827051-840e-43dc-a369-f6ed27b9aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd092b4c-cecb-4f60-8229-758d7b067157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image where you want to detect faces\n",
    "image = cv2.imread('image.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d88ab10d-7d62-4d73-b2c4-55015d95952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to grayscale (face detection works better on grayscale images)\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bf68be8-79de-48ad-89ef-9bc5034b67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69fe2611-e5ab-4e2f-84a5-0bea561d50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b2895d6-d290-4bc0-95e6-6af3d2cfaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw rectangles around the faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a2fe00d-c1dc-4973-aac1-387d4389868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the output image with detected faces\n",
    "cv2.imshow('Faces Detected', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad9644-7968-4ae0-ab26-3c11c4de4029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99437fc-6594-4a1c-8f4f-99ac75b331ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720e0e0-a349-45bb-96d5-91d4bef8a764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0008a-e5fc-40e6-b160-afc835c00544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e00d6a86-6635-454f-ae93-e7abcba5a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Draw rectangles around detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faafb08-6690-46df-9b41-efad5a51a091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56a323-1aa6-4d60-9649-95914d7a96c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da17815-4283-4d29-8328-d65ff947bd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a2e6f-e9e5-4734-8e55-2233995b1198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "800f1cb0-26cd-4c50-9b3e-ec561dc4ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import winsound  # for playing alarm sound on Windows, you can use other libraries for other platforms\n",
    "\n",
    "# Load Haar cascades for face and eye detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "distraction_start_time = None  # to track distraction duration\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect face\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    distraction_detected = False  # Track if the driver is distracted\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle around face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        # Region of interest for the eyes\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Detect eyes\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        if len(eyes) < 2:  # Assume driver is not looking straight ahead\n",
    "            distraction_detected = True\n",
    "            if distraction_start_time is None:\n",
    "                distraction_start_time = time.time()  # Start distraction timer\n",
    "        else:\n",
    "            distraction_start_time = None  # Reset timer if driver is attentive\n",
    "\n",
    "    # If distraction is detected for more than 3 seconds, trigger alarm\n",
    "    if distraction_detected:\n",
    "        if distraction_start_time is not None and time.time() - distraction_start_time > 3:\n",
    "            cv2.putText(frame, \"DISTRACTION DETECTED!\", (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            winsound.Beep(2000, 500)  # Play alarm sound (adjust for your system)\n",
    "    else:\n",
    "        distraction_start_time = None\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Driver Attention System', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7670c4c-4ded-46e6-92cb-b51a553918bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10375b00-c27a-466d-b606-d646231970c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702eccc-0788-45cb-bcad-8b78d2112276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30e9a5-6b20-4c65-874e-8d41696d82d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cdae97-19bd-44d2-a598-06805bed5757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Play/Pause music\n",
      "Next track\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Stop music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Play/Pause music\n",
      "Next track\n",
      "Play/Pause music\n",
      "Next track\n",
      "Play/Pause music\n",
      "Next track\n",
      "Play/Pause music\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pywhatkit as kit\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Music player control logic\n",
    "def control_music(gesture):\n",
    "    if gesture == \"one_finger\":\n",
    "        print(\"Play/Pause music\")\n",
    "        kit.playonyt(\"Shape of You\")  # Example of playing music on YouTube\n",
    "    elif gesture == \"two_fingers\":\n",
    "        print(\"Next track\")\n",
    "    elif gesture == \"palm_open\":\n",
    "        print(\"Stop music\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame with MediaPipe hands\n",
    "    result = hands.process(img_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_lms in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_lms, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Extract landmark coordinates for specific gesture detection\n",
    "            landmarks = hand_lms.landmark\n",
    "            \n",
    "            # Example: Gesture recognition using landmarks\n",
    "            # Detecting one finger gesture (finger #8 extended)\n",
    "            finger_tip = landmarks[8].y\n",
    "            finger_base = landmarks[6].y\n",
    "            \n",
    "            if finger_tip < finger_base:\n",
    "                control_music(\"one_finger\")  # Play/Pause\n",
    "\n",
    "            # Detecting two fingers (fingers #8 and #12 extended)\n",
    "            finger_tip_2 = landmarks[12].y\n",
    "            finger_base_2 = landmarks[10].y\n",
    "            \n",
    "            if finger_tip < finger_base and finger_tip_2 < finger_base_2:\n",
    "                control_music(\"two_fingers\")  # Next track\n",
    "\n",
    "            # Detecting palm open (all fingers extended)\n",
    "            if all(landmarks[i].y < landmarks[i - 2].y for i in [8, 12, 16, 20]):\n",
    "                control_music(\"palm_open\")  # Stop music\n",
    "    \n",
    "    # Display the result\n",
    "    cv2.imshow(\"Hand Gesture Control\", img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7243165d-79a9-466f-b806-3d3df2ecea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.4.34-cp312-cp312-win_amd64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (from mediapipe) (3.8.4)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in d:\\anaconda\\lib\\site-packages (from mediapipe) (4.25.5)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\anaconda\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax->mediapipe)\n",
      "  Using cached ml_dtypes-0.5.0-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: opt-einsum in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl (50.8 MB)\n",
      "Using cached sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached jax-0.4.34-py3-none-any.whl (2.1 MB)\n",
      "Using cached jaxlib-0.4.34-cp312-cp312-win_amd64.whl (55.3 MB)\n",
      "Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-win_amd64.whl (45.5 MB)\n",
      "Using cached ml_dtypes-0.5.0-cp312-cp312-win_amd64.whl (213 kB)\n",
      "Installing collected packages: opencv-contrib-python, ml-dtypes, absl-py, sounddevice, jaxlib, jax, mediapipe\n",
      "Successfully installed absl-py-2.1.0 jax-0.4.34 jaxlib-0.4.34 mediapipe-0.10.14 ml-dtypes-0.5.0 opencv-contrib-python-4.10.0.84 sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353722eb-56e1-42ca-b737-8b1b60fa2484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da131e34-a27e-4eb6-bdfe-a852240140eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd3289-8b3c-4e69-a184-149c8e570187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c800265d-ea04-430f-8b29-01180395acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "# Load face detection using OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize MediaPipe Hands for finger gesture detection\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Driver fatigue and distraction detection variables\n",
    "last_looked_at_road_time = time.time()\n",
    "fatigue_detected = False\n",
    "distraction_detected = False\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to play alarm\n",
    "def play_alarm():\n",
    "    winsound.Beep(1000, 1000)  # 1kHz beep for 1 second\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect face\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Check if face is detected\n",
    "    if len(faces) > 0:\n",
    "        # Draw rectangle around the face\n",
    "        (x, y, w, h) = faces[0]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Detect face landmarks to track eye openness (for drowsiness detection)\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "        gray_roi = gray[y:y + h, x:x + w]\n",
    "        \n",
    "        # Use MediaPipe to detect hand gestures (if available)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        # Process detected hand gestures\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Count the number of fingers raised (simple gesture recognition)\n",
    "                fingers_raised = 0\n",
    "                for i in [4, 8, 12, 16, 20]:  # Check landmarks for fingers\n",
    "                    if hand_landmarks.landmark[i].y < hand_landmarks.landmark[i - 2].y:\n",
    "                        fingers_raised += 1\n",
    "                \n",
    "                # If thumbs-up (1 finger raised), consider the driver alert\n",
    "                if fingers_raised == 1:\n",
    "                    cv2.putText(frame, \"Driver is alert (Thumbs Up)\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    last_looked_at_road_time = time.time()  # Reset the time when alert gesture is detected\n",
    "                else:\n",
    "                    # If no alert gesture, detect distraction\n",
    "                    if time.time() - last_looked_at_road_time > 5:  # 5 seconds without face in front\n",
    "                        distraction_detected = True\n",
    "                        cv2.putText(frame, \"Distraction Detected!\", (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        play_alarm()\n",
    "                    \n",
    "    else:\n",
    "        # If no face detected, assume the driver is distracted\n",
    "        distraction_detected = True\n",
    "        cv2.putText(frame, \"No Face Detected! Please pay attention!\", (20, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        play_alarm()\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('Driver Fatigue & Distraction Detection', frame)\n",
    "    \n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a080bef-8be7-4719-95d0-c2d7a76c3411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c70c25-bf27-4d3a-bd37-606349e0b577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5a9ed-6303-453b-ba53-b563f7db49ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a0327-5e71-4e28-8ff6-a33d4a8c66da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87690ca2-6e12-4c87-bc5b-e03201174383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Hands for hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# For drawing on the screen\n",
    "drawing_color = (0, 255, 0)  # Green color for drawing\n",
    "\n",
    "# Variables to store gestures\n",
    "gestures = []\n",
    "current_expression = \"\"\n",
    "\n",
    "# Define function to recognize numbers from hand gestures\n",
    "def recognize_gesture(landmarks):\n",
    "    # You can create a simple number gesture recognition based on hand landmarks.\n",
    "    # For now, just return a sample gesture for demonstration.\n",
    "    if landmarks[8].x > landmarks[4].x and landmarks[12].x > landmarks[4].x:\n",
    "        return \"1\"\n",
    "    if landmarks[8].x < landmarks[4].x and landmarks[12].x > landmarks[4].x:\n",
    "        return \"2\"\n",
    "    if landmarks[8].x > landmarks[4].x and landmarks[12].x < landmarks[4].x:\n",
    "        return \"3\"\n",
    "    return \"\"\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame for a mirror view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe hands\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    # Get hand landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw landmarks and connections\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Recognize the number or symbol being drawn\n",
    "            gesture = recognize_gesture(hand_landmarks.landmark)\n",
    "\n",
    "            if gesture:\n",
    "                gestures.append(gesture)\n",
    "\n",
    "    # Display the recognized expression\n",
    "    if len(gestures) > 0:\n",
    "        current_expression = \"\".join(gestures)\n",
    "\n",
    "    # Display the current expression on the frame\n",
    "    cv2.putText(frame, f\"Expression: {current_expression}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Evaluate the expression when Enter key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('\\r'):  # Enter key\n",
    "        try:\n",
    "            result = eval(current_expression)\n",
    "            cv2.putText(frame, f\"Result: {result}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        except Exception as e:\n",
    "            cv2.putText(frame, \"Error!\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the final frame\n",
    "    cv2.imshow(\"Gesture Calculator\", frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5621f-37bc-49b4-803d-3ed146882ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543023cd-76fc-4a4f-9373-379666805ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7180b58-fb01-425f-8cac-0be344e058a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0733bbd-dac5-4162-99c2-5a70bad501b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360065a-83b8-4937-b68f-e7f92582c02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6237b-0829-442c-85e3-77d62ed1fdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe2615-5924-4d8e-998a-e281f1917456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
